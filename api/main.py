import os
from dotenv import load_dotenv
from fastapi import FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from langchain_core.documents import Document
from openai import OpenAI
import numpy as np

# === 1. Carregar vari√°veis de ambiente ===
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
print(f"üîπ OPENAI_API_KEY encontrada: {'sim' if api_key else 'n√£o'}")
if not api_key:
    raise ValueError("‚ùå Nenhuma chave OPENAI_API_KEY encontrada no arquivo .env")

FAISS_PATH = "iso17025_faiss_qwen"

# === 2. Definir wrapper para embeddings CPU ===
class CPUEmbeddings(Embeddings):
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        # For√ßa uso de CPU para compatibilidade
        import torch
        torch.cuda.is_available = lambda : False
        self.model = SentenceTransformer(model_name, device='cpu')

    def embed_query(self, text: str) -> list[float]:
        return self.model.encode(text, convert_to_numpy=True).tolist()

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        return [self.model.encode(t, convert_to_numpy=True).tolist() for t in texts]

# === 3. Inicializar embeddings e FAISS ===
print("üîπ Carregando modelo de embeddings (CPU)...")
embeddings = CPUEmbeddings()

print("üîπ Carregando √≠ndice FAISS...")
faiss_index = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)

# === 4. Inicializar cliente da API OpenAI ===
print("üîπ Inicializando cliente OpenAI (responses API)...")
client = OpenAI(api_key=api_key)

# === 5. Configurar FastAPI ===
app = FastAPI(
    title="Assistente RAG para Consultoria em Qualidade Laboratorial",
    description="Sistema RAG aplicado √† norma ISO/IEC 17025:2017 para consultoria t√©cnica",
    version="1.0.0"
)

class QueryRequest(BaseModel):
    question: str

@app.post("/ask")
async def ask_rag(req: QueryRequest):
    """
    Endpoint principal do sistema RAG para consultoria em qualidade laboratorial.
    Recebe uma consulta, faz busca sem√¢ntica na base ISO 17025 e gera resposta fundamentada.
    """
    question = req.question.strip()
    if not question:
        return {"error": "Consulta vazia"}

    # === Recuperar requisitos mais relevantes da ISO 17025 ===
    retrieved_docs = faiss_index.similarity_search(question, k=5)
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])

    # === Montar prompt contextualizado para consultoria ===
    prompt = f"""
Voc√™ √© um consultor t√©cnico especializado em qualidade laboratorial que utiliza a norma ISO/IEC 17025:2017.
Responda √† consulta usando APENAS as informa√ß√µes do contexto fornecido dos requisitos da norma.

Instru√ß√µes:
- Seja preciso e t√©cnico
- Cite os n√∫meros das se√ß√µes quando relevante (ex: "conforme item 6.2.5", "se√ß√£o 7.4.1")
- Mantenha o foco na aplica√ß√£o pr√°tica para laborat√≥rios
- Se a informa√ß√£o n√£o estiver no contexto, indique claramente

Contexto da ISO/IEC 17025:2017:
{context}

Consulta do cliente: {question}

Resposta t√©cnica:
"""

    # === Chamar o modelo GPT para gerar resposta de consultoria ===
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=800
        )
        answer = response.choices[0].message.content.strip()
    except Exception as e:
        answer = f"‚ùå Erro ao gerar resposta de consultoria: {str(e)}"

    # === Retornar resultado da consultoria ===
    return {
        "question": question,
        "answer": answer,
        "context_used": [doc.page_content[:250] for doc in retrieved_docs],
        "documents_retrieved": len(retrieved_docs),
        "system_info": {
            "scenario": "Consultoria em Qualidade Laboratorial",
            "standard": "ISO/IEC 17025:2017",
            "method": "RAG (Retrieval-Augmented Generation)"
        }
    }

@app.get("/")
async def root():
    return {
        "message": "Assistente RAG para Consultoria em Qualidade Laboratorial est√° online! üöÄ",
        "scenario": "Consultoria t√©cnica especializada",
        "standard": "ISO/IEC 17025:2017",
        "technology": "RAG (Retrieval-Augmented Generation)",
        "endpoints": ["/ask", "/health"],
        "status": "ready"
    }

@app.get("/health")
async def health_check():
    """Endpoint para verificar a sa√∫de do sistema."""
    return {
        "status": "healthy",
        "faiss_index": "loaded",
        "embeddings_model": "Qwen3-Embedding-0.6B",
        "llm_model": "gpt-4o-mini",
        "documents_indexed": "156 requisitos ISO 17025"
    }
